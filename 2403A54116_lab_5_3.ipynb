{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIaJUnxo3HIyHAG2Z8BEZs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2403a54116-blip/sreenidhi-NLP/blob/main/2403A54116_lab_5_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9DdE_FlCsqz1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/arxiv_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset (first 1000 rows for safety)\n",
        "df = pd.read_csv('arxiv_data.csv', engine='python', nrows=1000)\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs (http, https, www)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove social media mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Remove special characters (keep only alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Normalize whitespace (reduce multiple spaces to single space)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to 'summaries' column\n",
        "df['processed_summaries'] = df['summaries'].apply(preprocess_text)\n",
        "\n",
        "# Preview results\n",
        "print(df[['summaries', 'processed_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3MTmKLXtAZb",
        "outputId": "3fbe26c9-85f8-496c-ff8b-d04913f91021"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                                 processed_summaries  \n",
            "0  stereo matching is one of the widely used tech...  \n",
            "1  the recent advancements in artificial intellig...  \n",
            "2  in this paper we proposed a novel mutual consi...  \n",
            "3  consistency training has proven to be an advan...  \n",
            "4  to ensure safety in automated driving the corr...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['processed_summaries'] = df['summaries'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "HRnnlqsZtCja"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # required in newer NLTK versions\n",
        "\n",
        "df['tokenized_summaries'] = df['processed_summaries'].apply(lambda x: word_tokenize(x))\n",
        "print(df[['processed_summaries', 'tokenized_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnygwZvdtJDi",
        "outputId": "c7a6dea9-4f3f-49c0-caa5-c29b2f156e54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 processed_summaries  \\\n",
            "0  stereo matching is one of the widely used tech...   \n",
            "1  the recent advancements in artificial intellig...   \n",
            "2  in this paper we proposed a novel mutual consi...   \n",
            "3  consistency training has proven to be an advan...   \n",
            "4  to ensure safety in automated driving the corr...   \n",
            "\n",
            "                                 tokenized_summaries  \n",
            "0  [stereo, matching, is, one, of, the, widely, u...  \n",
            "1  [the, recent, advancements, in, artificial, in...  \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...  \n",
            "3  [consistency, training, has, proven, to, be, a...  \n",
            "4  [to, ensure, safety, in, automated, driving, t...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure the stopwords resource is available\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define the set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from tokenized_summaries\n",
        "df['filtered_summaries'] = df['tokenized_summaries'].apply(\n",
        "    lambda tokens: [w for w in tokens if w.lower() not in stop_words]\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(df[['tokenized_summaries', 'filtered_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRssMmWftQo2",
        "outputId": "13baa389-6e2d-4f4f-eb6d-a8c4e73e2604"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 tokenized_summaries  \\\n",
            "0  [stereo, matching, is, one, of, the, widely, u...   \n",
            "1  [the, recent, advancements, in, artificial, in...   \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...   \n",
            "3  [consistency, training, has, proven, to, be, a...   \n",
            "4  [to, ensure, safety, in, automated, driving, t...   \n",
            "\n",
            "                                  filtered_summaries  \n",
            "0  [stereo, matching, one, widely, used, techniqu...  \n",
            "1  [recent, advancements, artificial, intelligenc...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure the WordNet corpus is available\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # optional, improves lemmatization coverage\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply lemmatization to each token list\n",
        "df['lemmatized_summaries'] = df['filtered_summaries'].apply(\n",
        "    lambda tokens: [lemmatizer.lemmatize(w) for w in tokens]\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(df[['filtered_summaries', 'lemmatized_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0gqXxJ5tVeu",
        "outputId": "8f09c5b5-0458-4b07-d592-d77e43bd46ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  filtered_summaries  \\\n",
            "0  [stereo, matching, one, widely, used, techniqu...   \n",
            "1  [recent, advancements, artificial, intelligenc...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                lemmatized_summaries  \n",
            "0  [stereo, matching, one, widely, used, techniqu...  \n",
            "1  [recent, advancement, artificial, intelligence...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rejoin lemmatized words into a single string\n",
        "df['clean_summaries'] = df['lemmatized_summaries'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "# Preview results\n",
        "print(df[['lemmatized_summaries', 'clean_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX7ObZYYtc4p",
        "outputId": "9426505c-6a0c-4513-a399-7062051ec349"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                lemmatized_summaries  \\\n",
            "0  [stereo, matching, one, widely, used, techniqu...   \n",
            "1  [recent, advancement, artificial, intelligence...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                     clean_summaries  \n",
            "0  stereo matching one widely used technique infe...  \n",
            "1  recent advancement artificial intelligence ai ...  \n",
            "2  paper proposed novel mutual consistency networ...  \n",
            "3  consistency training proven advanced semisuper...  \n",
            "4  ensure safety automated driving correct percep...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Reload dataset safely\n",
        "df = pd.read_csv('arxiv_data.csv', engine='python', nrows=1000)\n",
        "\n",
        "# Step 1: Tokenize\n",
        "df['tokenized_summaries'] = df['summaries'].apply(lambda x: word_tokenize(str(x)))\n",
        "\n",
        "# Step 2: POS tagging\n",
        "df['pos_summaries'] = df['tokenized_summaries'].apply(lambda tokens: pos_tag(tokens))\n",
        "\n",
        "# Step 3: NER\n",
        "def extract_entities(pos_tags):\n",
        "    chunked = ne_chunk(pos_tags)\n",
        "    entities = []\n",
        "    for subtree in chunked:\n",
        "        if hasattr(subtree, 'label'):\n",
        "            entity = \" \".join([token for token, pos in subtree.leaves()])\n",
        "            entities.append((entity, subtree.label()))\n",
        "    return entities\n",
        "\n",
        "# Apply to a small sample first to test\n",
        "df_sample = df.head(10).copy()\n",
        "df_sample['named_entities'] = df_sample['pos_summaries'].apply(extract_entities)\n",
        "\n",
        "print(\"Sample named entities:\")\n",
        "print(df_sample[['summaries', 'named_entities']])\n",
        "\n",
        "# Step 4: Frequency analysis (on sample for speed)\n",
        "all_entities = []\n",
        "for ents in df_sample['named_entities']:\n",
        "    all_entities.extend([entity for entity, label in ents])\n",
        "\n",
        "entity_freq = Counter(all_entities)\n",
        "print(\"\\nTop 10 most frequent named entities in sample:\")\n",
        "print(entity_freq.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE2eS11Z20OY",
        "outputId": "185dbc21-3fdd-4e76-f6a8-f371f5bc6987"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample named entities:\n",
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "5  High-quality training data play a key role in ...   \n",
            "6  Semantic segmentation of fine-resolution urban...   \n",
            "7  To mitigate the radiologist's workload, comput...   \n",
            "8  Generalising deep models to new data from new ...   \n",
            "9  The success of deep learning methods in medica...   \n",
            "\n",
            "                                      named_entities  \n",
            "0                                    [(Stereo, GPE)]  \n",
            "1  [(AI, ORGANIZATION), (AI, ORGANIZATION), (Euro...  \n",
            "2                                                 []  \n",
            "3  [(Consistency, GSP), (Atrial Segmentation, ORG...  \n",
            "4  [(Gaussian Mixture Models, PERSON), (GMM, ORGA...  \n",
            "5                         [(EdgeFlow, ORGANIZATION)]  \n",
            "6  [(Semantic, GPE), (CNNs, ORGANIZATION), (Visio...  \n",
            "7                      [(AutoEncoder, ORGANIZATION)]  \n",
            "8                                  [(Hence, PERSON)]  \n",
            "9                                                 []  \n",
            "\n",
            "Top 10 most frequent named entities in sample:\n",
            "[('EHT', 4), ('Mask', 3), ('AI', 2), ('GMM', 2), ('Morphological Snakes', 2), ('CNNs', 2), ('Stereo', 1), ('European', 1), ('Health', 1), ('Robustness', 1)]\n"
          ]
        }
      ]
    }
  ]
}